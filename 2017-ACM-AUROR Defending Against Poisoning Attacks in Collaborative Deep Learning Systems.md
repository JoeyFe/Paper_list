2017-ACM-AUROR Defending Against Poisoning Attacks in Collaborative Deep Learning Systems
===
> 引入了一种名为AUROR的统计机制来在生成准确模型的同时检测恶意用户。AUROR基于这样的观察：来自大多数诚实用户的指示性特征（最重要的模型特征）将表现出类似的分布，而来自恶意用户的特征将表现出异常分布。使用k-means在训练轮中对参与者进行聚类，并丢弃异常值，即来自超过阈值距离的小集群的贡献被删除。
### 背景和动机
**MNIST：** 具有43个输出类的手写字符数据集MNIST，当10%的参与者是恶意的时，攻击成功率高达99%。比较存在30%的恶意用户的与良性的数据集训练的模型，模型的平均准确率下降了24%。

**GTSRB：** 具有43个输出类[40]的德国交通符号图像(GTSRB)。攻击对特定目标有79%的成功率，当30%的用户是恶意的时，模型的准确率下降了约9%
`现有的防御方法需要完整的训练数据，协作学习或分布式机器学习拿不到完整的训练数据，需要在有没有一种方法在每一轮次的训练中检测恶意用户`
> 主要贡献
> * 测量协作学习对中毒攻击的敏感度
> * 提出防御机制AUROR
> * 评估实验数据
### 问题定义
用的这篇论文的隐私学习方法（PPDL），参与方上传的是一个$mask \ features$（就是梯度值）：
> [39] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1310–1321. ACM, 2015.
### 攻击模型
这边主要针对的目标攻击，

$S_I$是攻击成功的标识：使模型输出攻击者想要的目标输出

$D$是所有原输入

![image](https://user-images.githubusercontent.com/65484555/129557230-4508e238-3e82-4102-88c8-0b3197881be7.png)

攻击策略就是将参与训练的原数字标签做替换（就是做了一个标签翻转`label-flipping attack`）
###  数据集
手写图像(MNIST)和交通标志(GTSRB)
###  模型实现
1. 取前十轮迭代的梯度做`Kmeans`聚类，分成两个集群,两个集群中心距离超过$\alpha$,则标记为指示性特征。
2. 将`Kmeans`得到的指示性特征集群中用户数量少于一半的标记为恶性集群，如果某个用户有一半以上的样本被分到恶性集群中则标记这个用户为恶性用户
3. 从上一个步骤中标识的恶意用户中排除输入值，并用其余的$mask \ features$训练全局模型
### 实验结果
针对**MNIST**和**GTSRB**数据集，指示性特征都来自模型的最后一层，这证实了最后一层的参数很容易改变，因为与其他参数相比，它们对最终结果的影响最大。

> 针对AUROR：
> **第一种策略是减少恶意用户的比例：** 在某些情况下，减少恶意用户的数量可以逃避AUROR的检测机制，但这些中毒攻击产生的最终结果不能达到攻击者对数据进行错误分类的目标。（在MNIST上是100%检测的）
> **第二种策略是减少恶意用户训练集中的恶意样本的数量。** 对手可以减少其训练集中恶意样本的数量，以减少恶意用户和良性用户（α)之间的梯度距离(α）。然而，在避开AUROR的同时，平均攻击成功率和平均精度下降相对较低，无法实现对手的目标。

在有30%的恶意用户的情况下，AUROR训练的模型的准确性只下降了3%

